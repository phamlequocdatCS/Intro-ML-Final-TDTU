## Task 1
1. Understanding and comparing various Optimizer methods in training machine learning models.
2. Exploring "Continual Learning and Test Production" in machine learning.

## Optimizers
1. Gradient Descent
    
The gradient of a function $f$ is the vector field $\nabla f$ whose value at a point $p$ gives the direction and rate of fastest increase. If the gradient of the function is non-zero at $p$, the direction of the gradient is the direction in which the function increases most quickly from $p$, and the magnitude of the gradient is the rate of increase in that direction.

The gradient of a function $f(p)$ for $f:\mathbb{R}^n \rightarrow \mathbb{R}$ where $p=(x_ 1,x_ 2, ...,x_n)$ is defined as:

$$
\nabla f(p) = \begin{bmatrix}
\frac{\partial f}{\partial x_1}(p) \\
\vdots \\
\frac{\partial f}{\partial x_n}(p)
\end{bmatrix}
$$

A point where the gradient is the zero vector is known as a stationary point. A stationary point is a point where the function "stops" increasing or decreasing.
- A **local minima** is one where the derivative of the function changes from negative to positive.
- A **local maxima** is one where the derivative of the function changes from positive to negative.

Using the gradient, we can (hopefully) find a way to reach the local minima of an unknown function $F$, given $\nabla F$ (often the case in the real world). This is called **Gradient descent**.

Considering the neighborhood of a point $\bold{a}$, $F$ decreases fastest if one goes from $\bold{a}$ in the direction of the negative gradient of $F$ at $\bold{a}$, $-\nabla F(a)$.

If $\bold{a}_ {n+1} = \bold{a}_ n - \gamma \nabla F(\bold{a})$, then $F(\bold{a}_ n) \ge F(\bold{a}_ {n+1})$ for a small enough learning rate $\gamma \in \mathbb{R}_ +$. The term $\nabla F(\bold{a})$ is subtracted from $\bold{a}$ because we want to move against the gradient and toward the local minima.

The process of gradient descent starts with a guess $x_ 0$ for a local minima of $F$, then improve upon this guess for $x_ 1, x_ 2, ...$ such that $x_ {n+1} = x_ n - \gamma \nabla F(x), n \ge 0$. Hopefully, the sequence $(x_ n)$ converges to the desired local minimum.

In the context of **Multivariate Linear Regression**, given a linear equation 

$$
y = w_ 0 + w_ 1 x_ 1 + w_ 2 x_ 2 + ... + w_ n x_ n
$$
one can use Gradient Descent to find the weights $(w)$ given the input features $x$ and expected output $y$.

The function that we want to find the local minima of for this problem is a **loss function**, that is how far off the predicted output is from the actual output. There are many types of loss functions, one of which is the **Mean Squared Error** (MSE).

The **hypothesis** $\hat{y}$ given a learned set of weights $(w)$ is:

$$\hat{y} = \sum^{N} _ {i=0} (w_ j x _ {ij})$$

The **Mean Square Error** loss function of the expected output $y$ and the hypothesis $\hat{y}$
$$
L(w_ 0, w_ 1, ..., w_ n) = \frac{1}{N}\sum^{N} _ {i=0} (y_ i - \hat{y}_i)^2
$$
where $N$ is the number of samples in the training dataset.

The gradient $\nabla L$ is found by taking the derivative of the loss function

$$
\nabla L = 

\begin{bmatrix}
\frac{\partial L}{\partial w_0} \\
\vdots \\
\frac{\partial L}{\partial w_n}
\end{bmatrix}

=
\begin{bmatrix}
-\frac{2}{N} \sum^{N} _ {i=0} (y_ i - \hat{y}_i)            \\
-\frac{2}{N} \sum^{N} _ {i=0} (y_ i - \hat{y}_i)x_ {i1}     \\
\vdots \\
-\frac{2}{N} \sum^{N} _ {i=0} (y_ i - \hat{y}_i)x_ {in}    
\end{bmatrix}
$$

Finally, we update the weights via the formula 
$$w_ j := w_ j - \alpha \frac{\partial L}{\partial w_ j}, j = 0,1,...n$$
where $\alpha$ is the learning rate and $n$ is the number of features.

This process is repeated until the algorithm converges to a minimum, which is indicated by the gradient becoming very small (almost zero). The learning rate $\alpha$ determines how big the steps the algorithm takes towards the minimum are. If $\alpha$ is too large, the algorithm might overshoot the minimum and diverge. If $\alpha$ is too small, the algorithm will need many iterations to converge and might get stuck in a local minimum.

The main challenge with Gradient Descent is that it can get stuck in local minima. This is not a problem when the error surface (the Loss Gradient $\nabla L$) is convex, such as in Linear Regression. However, in Neural Networks, the error surface can have many local minima, and Gradient Descent can get stuck in a suboptimal solution. This is why modifications and alternatives to Gradient Descent, such as Stochastic Gradient Descent and Mini-Batch Gradient Descent, are often used in practice. These methods introduce randomness into the algorithm, which can help it escape from local minima.

2. Stochastic Gradient Descent (SGD)

Computing the gradient from the entire dataset can be computationally expensive, especially when dealing with large datasets. Since many real-world datasets follow a random probability distribution, it is possible to estimate the true gradient by taking a randomly selected subset of data. This is the principle behind **Stochastic Gradient Descent** (SGD).

Consider the loss function 

$$L(w) = \frac{1}{N}\sum^{N}_ {i=0} L_ i(w)$$

where $L_i(w)$ is the loss for the $i$-th sample given the weights $w$, and $N$ is the number of samples in the dataset.

In SGD, for each epoch, we shuffle the dataset, then update the weights for each example $i$ using the following rule:

$$w:=w-\alpha\nabla L_ i(w)$$

Here, $\alpha$ is the learning rate, and $\nabla L_ i(w)$ is the gradient of the loss for the $i$-th sample with respect to the weights.

The key difference between SGD and Gradient Descent is that in SGD, we perform the update on each training sample (or a small batch of samples), instead of updating the weights according to the entire dataset. This is denoted by $\nabla L_ i (w)$ versus $\frac{\partial L}{\partial w_ j} \in \nabla L$.

This approach has several advantages:
- **Computational efficiency**: SGD is much faster than Gradient Descent when dealing with large datasets, as it only needs to load one sample (or a small batch) into memory at a time.

- **Frequent updates**: With frequent updates, SGD can have a faster convergence and can also escape local minima more easily.

- **Ability to handle redundant data**: In some cases, the training data may contain redundant information. In such cases, SGD can reach the optimal solution faster by not using all the redundant data.

However, SGD also has some disadvantages:
- **Noisy updates**: Since SGD uses only one sample (or a small batch) to compute the gradient, the updates can be noisy, leading to a non-smooth path towards the minimum.
- **Requires tuning of learning rate**: The learning rate $\alpha$ often needs to be decreased over time to ensure convergence to the minimum, which adds an extra hyperparameter to tune.

Despite these challenges, SGD remains a popular optimization method due to its efficiency and simplicity.

3. Mini-Batch Gradient Descent

**Mini-Batch Gradient Descent** is a variant of Gradient Descent that splits the dataset into small subsets or "mini-batches". The algorithm then computes the gradient and updates the weights based on each of these mini-batches. It is a compromise between Gradient Descent and SGD.

Similar to SGD, we first shuffle the dataset, but then, we split the dataset into mini-batches of size $m$. For each mini-batch $B_k$, we compute the gradient of the average loss over the mini-batch with respect to the weights and update the weights as follows:

$$w:=w-\alpha\nabla \left(\frac{1}{m}\sum_{i \in B_k} L_ i(w)\right)$$

Here, $\alpha$ is the learning rate, and $\nabla \left(\frac{1}{m}\sum_{i \in B_k} L_ i(w)\right)$ is the gradient of the average loss over the mini-batch with respect to the weights.

This update is performed for each mini-batch, and the process is repeated for a number of epochs until the algorithm converges or a stopping criterion is met. 

The key difference with standard Gradient Descent and SGD is that the update is performed after each mini-batch (a subset of samples), not after the entire dataset (or one random sample in the case of SGD). This makes Mini-Batch Gradient Descent faster and more suitable for large datasets, while still providing more stable and less noisy updates than SGD. 

4. Momentum

**Momentum** is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction of the update vector of the past time step to the current update vector.

Also called the **heavy ball** method, it stems from an analogy to momentum in physics:

The weights vector $w$ can be thought of as a particle travelling through parameter space, and accumilate acceleration from the gradient of the loss. This particle would keep moving inthe same direction, instead of oscillating around the same stationery point. It can also escape a local minima if the momentum is high enough.

The update rule for the Momentum optimizer can be expressed as follows:

$$
v := \beta v - \alpha \nabla L(w)
$$
$$
w := w + v
$$

Here:
- $v$ is the velocity vector, which is an accumulation of the gradient elements.
- $\beta$ is the momentum term, which determines the amount of past gradients to retain. A typical value for $\beta$ is 0.9.
- $\alpha$ is the learning rate.
- $\nabla L(w)$ is the gradient of the loss function with respect to the weights.
- $w$ are the parameters of the model.

The momentum term $\beta$ is typically set to a value between 0 and 1, with a higher value resulting in a more stable optimization process. The larger the momentum term, the smoother the moving average, and the more resistant it is to changes in the gradients.

5. Adaptive Gradient Algorithm (Adagrad)

Adagrad is a gradient-based optimization algorithm that adapts the learning rate for each parameter during the optimization process, based on the past gradients observed for that parameter. It performs larger updates (high learning rates) for parameters related to infrequent features and smaller updates (low learning rates) for frequent ones. This makes it well-suited for dealing with sparse data.

The update rule for Adagrad is:

$$
w := w - \frac{\alpha}{\sqrt{G + \epsilon}} \cdot \nabla L(w)
$$

where $w$ are the parameters, $\alpha$ is the learning rate, $G$ is the sum of squares of past gradients, $\nabla L(w)$ is the gradient of the loss function, and $\epsilon$ is a small smoothing term to avoid division by zero (typically on the order of $1e-8$).

or written as per-parameter updates, 

$$
w_ j := w_ j - \frac{\alpha}{\sqrt{G_ {j,j} + \epsilon}} \cdot g_ j
$$

$G_ {jj}$ is the sum of the squares of the past gradients with respect to $w_j$ up to the current time step. It's the $j$-th diagonal element of the matrix $G$.

$G$ is a diagonal matrix where each diagonal element $j, j$ is the sum of the squares of the gradients w.r.t. $w_ j$ up to time step $t$, while all non-diagonal elements are 0. Here $w_ j$ is the $j$-th parameter of the model.

Mathematically, each element in the diagonal $G_ {j,j}$ is calculated as:

$$
G_{j,j} = \sum^{t}_{\tau=1} g_{\tau, j}^2
$$

where $g_{\tau, i}$ is the gradient at time step $\tau$ w.r.t. $w_ j$.

The $G$ matrix is used to adaptively adjust the learning rates of the model parameters. The learning rate for each weight is scaled by the inverse square root of the sum of the squares of past gradients for that weight. This means that weights associated with frequently occurring features (large gradients) get smaller updates, and weights associated with infrequent features (small gradients) get larger updates.

6. Root Mean Square Propagation (RMSProp)

**RMSProp** is an extension of gradient descent and the AdaGrad version of gradient descent that uses a decaying average of past squared gradients to adapt the step size for each parameter. The use of a decaying moving average allows the algorithm to forget early gradients and focus on the most recently observed partial gradients.

The update rule for RMSProp is:

$$
E[g^2]_t = \beta E[g^2]_{t-1} + (1-\beta)g_t^2
$$
$$
w := w - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \cdot g_t
$$

where $E[g^2]_t$ is the decaying average of past squared gradients, $\beta$ is the decay rate (forget rate), $g_t$ is the current gradient, and other symbols have the same meaning as before.

To calculate $E[g^2]_t$, we take the square of each past gradient, and then compute a running (or moving) average of these squared gradients. This running average is a weighted average where more recent squared gradients have more weight, and the weight of older squared gradients decays exponentially over time. This is controlled by the decay rate $\beta$, usually 0.9.

7. Adaptive Moment Estimation (Adam)

**Adaptive Moment Estimation** (Adam) is an update to the RMSProp optimizer by combining it with the main feature of the Momentum method. Running averages with exponential forgetting of both the gradients and the second moments of the gradients are used.

The components of the update rule for Adam are:

**Momentum Component**: 
    
First, an estimate of the first moment (the mean) of the gradients, which is analogous to the momentum term in the Momentum method. This is represented by $m_ t$ in the Adam update rule:

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t
$$

Here, $m_{t-1}$ is the estimate of the first moment at the previous time step, $\beta_1$ is the decay rate for the first moment estimates (typically set to 0.9), and $g_t$ is the gradient at the current time step.

**RMSProp Component**

Next an estimate of the second moment (the uncentered variance) of the gradients, which is a key component of the RMSProp method. This is represented by $v_t$ in the Adam update rule:

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2
$$

Here, $v_{t-1}$ is the estimate of the second moment at the previous time step, $\beta_2$ is the decay rate for the second moment estimates (typically set to 0.999), and $g_t$ is the gradient at the current time step.

The update rule for Adam is as follows:

$$
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
$$
$$
\hat{v}_t = \frac{v_t}{1-\beta_2^t}
$$
$$
w = w - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t
$$

where $m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, $\hat{m}_t$ and $\hat{v}_t$ are bias-corrected estimates of the first and second moments, $\beta_1$ and $\beta_2$ are exponential decay rates for the moment estimates.

8. Summary

    1. **Gradient Descent (GD)**: This is the basic form of gradient descent, where we compute the gradient using the entire dataset and update the weights accordingly. It's computationally expensive for large datasets and can be slow to converge.

    2. **Stochastic Gradient Descent (SGD)**: In SGD, we update the weights using the gradient computed from a single randomly chosen data point. This makes it faster and more computationally efficient than GD, especially for large datasets. However, the updates can be noisy, leading to a non-smooth path towards the minimum.

    3. **Mini-Batch Gradient Descent**: This is a compromise between GD and SGD. We compute the gradient and update the weights based on a small random subset of the data (a mini-batch). This can lead to more stable and less noisy updates than SGD, and convergence is often faster. It can also take advantage of vectorized operations for additional computational efficiency.

    4. **Momentum**: This method helps accelerate SGD in the relevant direction and dampens oscillations by adding a fraction of the update vector of the past time step to the current update vector. It's like rolling a ball down a hill, the ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (analogous to a lower learning rate).

    5. **Adagrad (Adaptive Gradient Algorithm)**: Adagrad adapts the learning rate for each parameter during the optimization process, performing larger updates for infrequent parameters and smaller updates for frequent ones. It uses a different learning rate for every parameter $w_j$ at every time step $t$, based on the past gradients that have been computed for $w_j$.

    6. **RMSProp (Root Mean Square Propagation)**: RMSProp is an extension of Adagrad that deals with its radically diminishing learning rates. It uses a moving average of squared gradients to normalize the gradient itself. It's well-suited for non-stationary objectives and problems with very noisy and/or sparse gradients.

    7. **Adam (Adaptive Moment Estimation)**: Adam is an update to the RMSProp optimizer. In addition to storing an exponentially decaying average of past squared gradients like RMSProp, Adam also keeps an exponentially decaying average of past gradients, similar to momentum. This makes the method invariant to diagonal rescale of the gradients.


Sources: 
1. [Wikipedia - Gradient](https://en.wikipedia.org/wiki/Gradient)
2. [Wikipedia - Stationary point](https://en.wikipedia.org/wiki/Stationary_point)
3. [Wikipedia - Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)
4. [Wikipedia - Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
5. [Baeldung - Differences Between Gradient, Stochastic and Mini Batch Gradient Descent](https://www.baeldung.com/cs/gradient-stochastic-and-mini-batch)
6. [Analytics Vidhya - Why use the momentum optimizer with minimal code example](https://medium.com/analytics-vidhya/why-use-the-momentum-optimizer-with-minimal-code-example-8f5d93c33a53)
7. [databricks - adagrad](https://www.databricks.com/glossary/adagrad)
8. [OpenGenus IQ - adagrad](https://iq.opengenus.org/adagrad/)
9. [Ruder, Sebastian - An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747)
10. [Duchi, John, Elad Hazan, and Yoram Singer - Adaptive subgradient methods for online learning and stochastic optimization](https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
11. [OpenGenus IQ - RMSprop](https://iq.opengenus.org/rmsprop/)
12. [Papers with code - RMSProp](https://paperswithcode.com/method/rmsprop)